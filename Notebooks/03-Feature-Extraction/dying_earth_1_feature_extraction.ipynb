{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "books_songs =  [\n",
    "    \"Attractive and Detractive Hyperordnets\",\n",
    "    \"the Lost Book of Kells\",\n",
    "    \"The Opal, the Pearl and the Peacock\",\n",
    "    \"'The Opal, the Pearl and the Peacock\",\n",
    "    \"Demonlands\",\n",
    "    \"Killings and Mortefactions\",\n",
    "    \"Attractive and Detractive Hyperordnets\",\n",
    "    \"Procedural Suggestions in Time of Risk\",\n",
    "    \"the Tomes of Kae\",\n",
    "    \"the Word of Pansiu\"\n",
    "]\n",
    "\n",
    "artifacts_objects = [\n",
    "    \"Live Boots\",\n",
    "    \"the Live Boots\",\n",
    "    \"Cyclopedia\",\n",
    "    \"the Expansible Egg\",\n",
    "    \"Chair of Knowledge\",\n",
    "    \"Scintillant Dagger\",\n",
    "    \"Boots\",\n",
    "    \"Mechanismus\",\n",
    "    \"Rune\",\n",
    "    \"Egg\",\n",
    "    \"Sphere\"\n",
    "]\n",
    "\n",
    "spells = [\n",
    "    \"the Omnipotent Sphere\",\n",
    "    \"the Call to the Violent Cloud\",\n",
    "    \"the Excellent Prismatic Spray\",\n",
    "    \"Mantle of Stealth\",\n",
    "    \"the Spell of the Slow Hour\",\n",
    "    \"Four Directions\",\n",
    "    \"Second Hypnotic Spell\",\n",
    "    \"The Charm of Untiring Nourishment\",\n",
    "    \"Critique of the Chill\",\n",
    "    \"Gyrator\",\n",
    "    \"Lumen\",\n",
    "    \"the Call to the Violent Cloud\",\n",
    "    \"the Spell of the Omnipotent Sphere\"\n",
    "]\n",
    "\n",
    "characters = [\n",
    "    \"Pansiu's\",\n",
    "    \"Guyal\",\n",
    "    \"Kandive\",\n",
    "    \"Kandive the Golden\",\n",
    "    \"Guyal of Sfere\", \n",
    "    \"Liane the Wayfarer\", \n",
    "    \"Mazirian\", \n",
    "    \"Turjan\", \n",
    "    \"T'sais\", \n",
    "    \"Ulan Dhor\", \n",
    "    \"Elai\", \n",
    "    \"Etarr\", \n",
    "    \"Prince Kandive\", \n",
    "    \"Pandelume\", \n",
    "    \"Rogol Domedonfors\", \n",
    "    \"Shierl\", \n",
    "    \"T'sain\",\n",
    "    \"Cazdal\",\n",
    "    \"Javanne\",\n",
    "    \"Kerlin\",\n",
    "    \"the Lake Lord\",\n",
    "    \"the Arch-Necromancer Phandaal\",\n",
    "    \"Pansiu\",\n",
    "    \"Melantine\",\n",
    "    \"Voyevode\",\n",
    "    \"Kandive the Golden\",\n",
    "    \"Blikdak\",\n",
    "    \"Laccodel\",\n",
    "    \"Mad King Shin\",\n",
    "    \"Lycurgat\",\n",
    "    \"Saponid\"\n",
    "]\n",
    "\n",
    "locations = [\n",
    "    \"Ampridatvir\",\n",
    "    \"Erze Damath\",\n",
    "    \"Kaiin\",\n",
    "    \"Sanctuary of the Pelerines\",\n",
    "    \"Ascolais\",\n",
    "    \"The Scaum Valley\",\n",
    "    \"The Forest of Tantrevalles\",\n",
    "    \"Ruins of Old Romarth\",\n",
    "    \"The Cleft of the Earth\",\n",
    "    \"Overworld\",\n",
    "    \"Azenomei\",\n",
    "    \"Ulan Dhor\",\n",
    "    \"Almery\",\n",
    "    \"Embelyon\",\n",
    "    \"the Land of the Falling Wall\",\n",
    "    \"Sfere\",\n",
    "    \"Thamber\",\n",
    "    \"Kaiin\",\n",
    "    \"Miir\",\n",
    "    \"Ascolais\",\n",
    "    \"Efred\",\n",
    "    \"Jeldred\",\n",
    "    \"Saponce\",\n",
    "    \"Maurenron Range\",\n",
    "    \"Porphiron Scar\",\n",
    "    \"Omona Gap\",\n",
    "    \"East Almery\",\n",
    "    \"Bautiku\",\n",
    "    \"Tenebrosa\",\n",
    "    \"Kalu\",\n",
    "    \"Fauvune\",\n",
    "    \"Cansapara\",\n",
    "    \"South Almery\",\n",
    "    \"Ariventa\",\n",
    "    \"Sanreale\",\n",
    "    \"Tanvilkat\",\n",
    "    \"the Old Town\",\n",
    "    \"Ampridatvir\",\n",
    "    \"Mel-Palusas\",\n",
    "    \"Fer Aquila\",\n",
    "    \"Carchasel\",\n",
    "    \"Derna\",\n",
    "    \"Regatta\",\n",
    "    \"Carchesel\",\n",
    "    \"Scaum\",\n",
    "    \"Liane\",\n",
    "    \"Thorsingol\",\n",
    "    \"Peilvemchal Torrent\",\n",
    "    \"the Porphiron Scar\",\n",
    "    \"the River Scaum\",\n",
    "    \"the Ide of Kauchique\",\n",
    "    \"the Cape of Sad Remembrance\",\n",
    "    \"Thamber Meadow\",\n",
    "    \"the Lake of Dreams\",\n",
    "    \"G'Vasan\",\n",
    "    \"Melantine\"\n",
    "]\n",
    "\n",
    "facilities = [\n",
    "    \"Mansion of Chun the Unavoidable\",\n",
    "    \"the Place of Whispers\",\n",
    "    \"the Tower of Fate\",\n",
    "    \"the Tower of the Screaming Ghost\",\n",
    "    \"the Tower of Trumpets\",\n",
    "    \"the Museum of Man\",\n",
    "    \"the Cognative Repository\",\n",
    "    \"Temple\",\n",
    "    \"Caseboard\",\n",
    "    \"Museum of Man\"\n",
    "]\n",
    "\n",
    "events = [\n",
    "    \"the Black Sabbath\",\n",
    "    \"the Dance of the Fourteen Silken Movements\",\n",
    "    \"Dawn\"\n",
    "]\n",
    "\n",
    "norps = [\n",
    "    \"the Signs of the Aumoklopelastianic Cabal\",\n",
    "    \"Ghost-takers\",\n",
    "    \"Norns\",\n",
    "    \"Gaun\",\n",
    "    \"The Green Legion of Valdaran the Just\",\n",
    "    \"the Grays of Ampridatvir\",\n",
    "    \"Saponids\",\n",
    "    \"Saponid\",\n",
    "    \"the Saponids of Saponce\",\n",
    "    \"Ampridatvians\",\n",
    "    \"Grays\",\n",
    "    \"Raiders\",\n",
    "    \"the Green Legion\",\n",
    "    \"Green Legion\",\n",
    "    \"the Forty Kades\",\n",
    "    \"the Sherit Empire\",\n",
    "    \"Merioneth\",\n",
    "    \"the Gray Sorcerers\"\n",
    "]\n",
    "\n",
    "creatures = [\n",
    "    \"Deodand\",\n",
    "    \"Vile Green Demon\",\n",
    "    \"Thrang\",\n",
    "    \"Deodands\"\n",
    "    \n",
    "]\n",
    "\n",
    "other = [\n",
    "    \"Poh\",\n",
    "    \"Mark\",\n",
    "    \"Green\",\n",
    "    \"Lethargy\",\n",
    "    \"Golden\",\n",
    "    \"Aye\",\n",
    "    \"Pulchritude\",\n",
    "    \"the Mechanismus sixty\",\n",
    "    \"The Curator guards the Museum of Man\",\n",
    "    \"Curator or Museum\",\n",
    "    \"Gap\",\n",
    "    \"Wayfarer\"\n",
    "]\n",
    "\n",
    "correction_dict = {\n",
    "    \"BOOK_SONG\": books_songs,\n",
    "    \"ARTIFACT_OBJECT\": artifacts_objects,\n",
    "    \"SPELL\": spells,\n",
    "    \"PERSON\": characters,\n",
    "    \"LOC\": locations,\n",
    "    \"FAC\": facilities,\n",
    "    \"EVENT\": events,\n",
    "    \"NORP\": norps,\n",
    "    \"CREATURE\": creatures,\n",
    "    \"OTHER\": other\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def open_book(filename):\n",
    "    with open(\"../../Resources/Cleaned/\"+filename+\".txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def add_book_to_df(book, book_title):\n",
    "    # Split the book text into paragraphs\n",
    "    paragraphs = book.split('\\n')\n",
    "    \n",
    "    # Clean each paragraph by removing extra whitespace and trimming\n",
    "    paragraphs = [re.sub(r'\\s+', ' ', para.strip()) for para in paragraphs if para.strip()]\n",
    "\n",
    "    # Create a DataFrame with two columns: book title and the paragraph text\n",
    "    df = pd.DataFrame({'Title': [book_title] * len(paragraphs), 'Text': paragraphs})\n",
    "    return df\n",
    "\n",
    "def correct_entity_type(entity_text, correction_dict):\n",
    "    # Normalize the entity text (lowercase, remove extra spaces, handle special chars)\n",
    "    entity_text_normalized = re.sub(r'\\s+', ' ', entity_text).lower().strip(\" '\\\"\")\n",
    "\n",
    "    for category, names in correction_dict.items():\n",
    "        # Normalize and prepare the names in the dictionary\n",
    "        normalized_names = [re.sub(r'\\s+', ' ', name).lower().strip(\" '\\\"\") for name in names]\n",
    "        \n",
    "        if entity_text_normalized in normalized_names:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "def find_entities_in_paragraph(paragraph, entities):\n",
    "    entities_in_paragraph = set()\n",
    "    for ent_text, ent_type in entities:\n",
    "        if ent_text in paragraph:\n",
    "            entities_in_paragraph.add((ent_text, ent_type))\n",
    "    return list(entities_in_paragraph)\n",
    "\n",
    "def dialogue_to_df(text):\n",
    "    pattern = r'\"([^\"]*)\"'\n",
    "    dialogues = re.findall(pattern, text)\n",
    "    df_dialogues = pd.DataFrame(dialogues, columns=['Dialogue'])\n",
    "    return df_dialogues\n",
    "\n",
    "def key_phrase_extractor(text, n=1):\n",
    "    additional_stopwords = {'said', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\"}\n",
    "    custom_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "    # Tokenize the text into words, remove punctuation with regex\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', text))\n",
    "\n",
    "    # Remove stop words and convert to lowercase\n",
    "    words_without_stopwords = [word.lower() for word in words if word.lower() not in custom_stopwords]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = ngrams(words_without_stopwords, n)\n",
    "    n_grams = [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "    # Count the frequency of each n-gram\n",
    "    frequency = Counter(n_grams)\n",
    "\n",
    "    # Get the top N key phrases\n",
    "    N = 100\n",
    "    key_phrases = frequency.most_common(N)\n",
    "\n",
    "    # Create a DataFrame from the top key phrases\n",
    "    df = pd.DataFrame(key_phrases, columns=['phrase', 'count'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_character(entity):\n",
    "    character_types = {'PERSON'}\n",
    "    return entity[1] in character_types\n",
    "\n",
    "def is_location(entity):\n",
    "    location_types = {'LOC'}\n",
    "    return entity[1] in location_types\n",
    "\n",
    "\n",
    "def df_to_csv(df, filename):\n",
    "    df.to_csv(\"../../Resources/Cleaned/\"+filename+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open_book(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "entities = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append((ent.text, ent.label_))\n",
    "\n",
    "df = pd.DataFrame(entities, columns=[\"Entity\", \"Entity Type\"])\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entity_types = df[\"Entity Type\"].unique()\n",
    "\n",
    "# Display the unique entity types\n",
    "print(unique_entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_entities = df[df[\"Entity Type\"] == \"PERSON\"]\n",
    "\n",
    "# Print the filtered entities\n",
    "print(language_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df = add_book_to_df(book, \"The Dying Earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set()\n",
    "for sentence in doc.sentences:\n",
    "    for entity in sentence.ents:\n",
    "        # Add entity text to the set, this automatically removes duplicates\n",
    "        entities.add(entity.text)\n",
    "\n",
    "# Displaying the comprehensive list of entities\n",
    "for entity in sorted(entities):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_entities = []\n",
    "\n",
    "for ent in doc.entities:\n",
    "    corrected_type = correct_entity_type(ent.text, correction_dict)\n",
    "    \n",
    "    # Use the corrected type if available; otherwise, use the original type\n",
    "    \n",
    "    ent_type = corrected_type if corrected_type else ent.type\n",
    "    corrected_entities.append((ent.text, ent_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of unwanted entities\n",
    "unwanted_entities = {\n",
    "    (\"TURJAN\", \"PERSON\"),\n",
    "    (\"Aghast\", \"PERSON\"),\n",
    "    (\"Ampridatvirian\", \"NORP\"),\n",
    "    (\"Azvan the Astronomer\", \"PERSON\"),\n",
    "    (\"Boots\", \"ARTIFACT_OBJECT\"),\n",
    "    (\"Carchasel\", \"LOC\"),\n",
    "    (\"Caseboard\", \"FAC\"),\n",
    "    (\"Castellan\", \"NORP\"),\n",
    "    (\"Castellan\", \"PERSON\"),\n",
    "    (\"Castellan's\", \"PERSON\"),\n",
    "    (\"Dandanflores\", \"NORP\"),\n",
    "    (\"Dawn\", \"EVENT\"),\n",
    "    (\"Deodands\", \"CREATURE\"),\n",
    "    (\"Dhor\", \"PERSON\"),\n",
    "    (\"Dusty\", \"PERSON\"),\n",
    "    (\"East\", \"LOC\"),\n",
    "    (\"Egg\", \"ARTIFACT_OBJECT\"),\n",
    "    (\"Elai's\", \"PERSON\"),\n",
    "    (\"Etarr the Masked\", \"PERSON\"),\n",
    "    (\"Ethodea\", \"NORP\"),\n",
    "    (\"Falling Wall\", \"LOC\"),\n",
    "    (\"Felon\", \"PERSON\"),\n",
    "    (\"Four Directions\", \"SPELL\"),\n",
    "    (\"Gaun\", \"NORP\"),\n",
    "    (\"Gauns\", \"PERSON\"),\n",
    "    (\"Golickan Kodek the Conqueror\", \"PERSON\"),\n",
    "    (\"Gray\", \"PERSON\"),\n",
    "    (\"Guyal of Sfere\", \"PERSON\"),\n",
    "    (\"Gyrator\", \"SPELL\"),\n",
    "    (\"Hideous\", \"PERSON\"),\n",
    "    (\"I am Chun the Unavoidable\", \"PERSON\"),\n",
    "    (\"Kandive the Golden\", \"PERSON\"),\n",
    "    (\"Kerlin the Curator\", \"PERSON\"),\n",
    "    (\"Kerlin's\", \"PERSON\"),\n",
    "    (\"Liane the Wayfarer\", \"PERSON\"),\n",
    "    (\"MAGICIAN\", \"PERSON\"),\n",
    "    (\"MAZIRIAN\", \"PERSON\"),\n",
    "    (\"Mazirian the Magician\", \"PERSON\"),\n",
    "    (\"Moon\", \"LOC\"),\n",
    "    (\"this Temple of Pansiu\", \"FAC\"),\n",
    "    (\"Prince Kandive the Golden\", \"PERSON\"),\n",
    "    (\"Porrina\", \"PERSON\"),\n",
    "    (\"Pubescentarium\", \"FAC\"),\n",
    "    (\"Raider\", \"PERSON\"),\n",
    "    (\"Regatta\", \"LOC\"),\n",
    "    (\"Rogol Domedonfors'\", \"PERSON\"),\n",
    "    (\"Sergeant-Reader of the Litany\", \"PERSON\"),\n",
    "    (\"South\", \"LOC\"),\n",
    "    (\"Temple\", \"FAC\"),\n",
    "    (\"Magician\", \"PERSON\"),\n",
    "    (\"Turjan of Miir\", \"PERSON\"),\n",
    "    (\"Tâ€™sais\", \"PERSON\"),\n",
    "    (\"ULAN\", \"PERSON\"),\n",
    "    (\"Ulan Dhor\", \"PERSON\"),\n",
    "    (\"Ulan Dhor's\", \"PERSON\"),\n",
    "    (\"Uncle Ludowik's\", \"PERSON\"),\n",
    "    (\"Uncle Ludowik\", \"PERSON\"),\n",
    "    (\"earth\", \"LOC\"),\n",
    "    (\"the Dance of the Fourteen Silken Movements\", \"EVENT\"),\n",
    "    (\"the Tower  of Fate\", \"FAC\"),\n",
    "}\n",
    "\n",
    "# Define a set of unwanted types\n",
    "unwanted_types = {'DATE', 'TIME', 'CARDINAL', 'ORDINAL', 'LAW', 'QUANTITY', 'BOOK_SONG', 'OTHER'}\n",
    "\n",
    "# Initialize the list for the updated entities\n",
    "updated_entities = []\n",
    "\n",
    "# Iterate over corrected_entities to rename, correct format, and filter\n",
    "for entity in corrected_entities:\n",
    "    entity_text, entity_type = entity\n",
    "\n",
    "    # Check for the specific entity \"Bay the Cape of Sad Remembrance\" to rename\n",
    "    if entity == (\"Bay the Cape of Sad Remembrance\", \"FAC\"):\n",
    "        entity_text = \"Cape of Sad Remembrance\"\n",
    "\n",
    "    # Correct the format for the entity '\"The Green Legion of Valdaran the Just'\n",
    "    if entity == ('\"The Green Legion of Valdaran the Just', 'NORP'):\n",
    "        entity_text = \"The Green Legion of Valdaran the Just\"\n",
    "\n",
    "    # Change (\"Olek\", \"PERSON\") to (\"Olek'hnit\", \"NORP\")\n",
    "    if entity == (\"Olek\", \"PERSON\"):\n",
    "        entity_text = \"Olek'hnit\"\n",
    "        entity_type = \"NORP\"\n",
    "\n",
    "    # Create a new tuple with the possibly updated text and type\n",
    "    updated_entity = (entity_text, entity_type)\n",
    "\n",
    "    # Add the entity to the list if it's not unwanted\n",
    "    if updated_entity[1] not in unwanted_types and updated_entity not in unwanted_entities:\n",
    "        updated_entities.append(updated_entity)\n",
    "\n",
    "# Now updated_entities contains your required entities\n",
    "entities = updated_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entities = list(set(entities))\n",
    "\n",
    "# Displaying the unique named entities\n",
    "for ent_text, ent_type in sorted(unique_entities):\n",
    "    print(f'Entity: (\"{ent_text}\", \"{ent_type}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "# Function to process a paragraph and return matching entities\n",
    "def extract_matching_entities(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    paragraph_entities = set((ent.text, ent.type) for sent in doc.sentences for ent in sent.ents)\n",
    "    # Return entities that are in both paragraph_entities and unique_entities\n",
    "    return paragraph_entities.intersection(unique_entities)\n",
    "\n",
    "# Apply the function to each paragraph and create a new column with the matched entities\n",
    "de_df['Entities'] = de_df['Text'].apply(extract_matching_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = pd.DataFrame(unique_entities, columns=[\"Entity_Name\", \"Entity_Type\"])\n",
    "ent_df['Entity_Name'] = ent_df['Entity_Name'].str.title()\n",
    "\n",
    "# Remove duplicates\n",
    "de_ent_df = ent_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ent_df[\"Entity_Type\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_key_phrase_df = key_phrase_extractor(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dialogue_df = dialogue_to_df(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dialogue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df_para = de_df.drop(columns=['Entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df_para.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(de_df, \"dying_earth1_paragraphs_entities\")\n",
    "df_to_csv(de_dialogue_df, \"dying_earth1_dialogue\")\n",
    "df_to_csv(de_key_phrase_df, \"dying_earth1_key_phrases\")\n",
    "df_to_csv(de_ent_df, \"dying_earth1_entities\")\n",
    "df_to_csv(de_df_para, \"dying_earth1_paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df['Entities'] = de_df['Entities'].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entities(entities_set):\n",
    "    return {(entity, type) for entity, type in entities_set if not (entity == \"Earth\" and type == \"LOC\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# Assuming de_df is your DataFrame and already loaded with the 'Entities' column filled as per your function\n",
    "\n",
    "# Define entity types and their corresponding colors\n",
    "entity_types = ['PERSON', 'LOC', 'ARTIFACT_OBJECT', 'FAC', 'NORP', 'SPELL', 'CREATURE', 'EVENT']\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange', 'purple']\n",
    "color_map = dict(zip(entity_types, colors))\n",
    "\n",
    "# Create a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their type\n",
    "for _, row in de_df.iterrows():\n",
    "    filtered_entities = filter_entities(row['Entities'])\n",
    "    for entity, entity_type in filtered_entities:\n",
    "        G.add_node(entity, type=entity_type)\n",
    "\n",
    "# Add edges (for simplicity, connecting all entities within the same text, except (\"Earth\", \"LOC\"))\n",
    "for _, row in de_df.iterrows():\n",
    "    filtered_entities = filter_entities(row['Entities'])\n",
    "    entities = [entity for entity, _ in filtered_entities]\n",
    "    for source, target in combinations(entities, 2):\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "# Position the nodes using a layout to bring outliers closer\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "# Prepare plotly graph\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_color = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_color.append(color_map.get(G.nodes[node]['type'], 'grey'))\n",
    "    node_text.append(node)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=node_color,\n",
    "        line_width=2))\n",
    "\n",
    "# Create layout for the graph\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='<br>Network graph of entities in \"The Dying Earth\"',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
    "fig.update_layout(\n",
    "    width=1000,  # Set the width of the plot\n",
    "    height=1000)\n",
    "\n",
    "# Code to display the graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty lists in 'Entities' with NaN, so they are handled properly when exploding\n",
    "de_df['Entities'] = de_df['Entities'].apply(lambda x: x if x else float('nan'))\n",
    "\n",
    "# Explode the 'Entities' column\n",
    "exploded_df = de_df.explode('Entities')\n",
    "\n",
    "# Drop rows with NaN in 'Entities' (which were originally empty lists)\n",
    "exploded_df = exploded_df.dropna(subset=['Entities'])\n",
    "\n",
    "# Split the tuples in 'Entities' into two columns, 'Entity' and 'Type'\n",
    "exploded_df[['Entity', 'Type']] = pd.DataFrame(exploded_df['Entities'].tolist(), index=exploded_df.index)\n",
    "\n",
    "# Drop the original 'Entities' column\n",
    "exploded_df = exploded_df.drop('Entities', axis=1)\n",
    "\n",
    "# Filter out rows where 'Entity' is 'Earth' and 'Type' is 'LOC'\n",
    "exploded_df = exploded_df[~((exploded_df['Entity'] == 'Earth') & (exploded_df['Type'] == 'LOC'))]\n",
    "\n",
    "exploded_df.loc[exploded_df['Entity'] == \"Land of the Falling Wall\", 'Type'] = \"LOC\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_format = {\n",
    "    'PERSON': 'Person',\n",
    "    'LOC': 'Location',\n",
    "    'ARTIFACT_OBJECT': 'Artifact or Object',\n",
    "    'FAC': 'Facility',\n",
    "    'NORP': 'Nationality or Religious or Political group',\n",
    "    'SPELL': 'Spell',\n",
    "    'CREATURE': 'Creature',\n",
    "    'EVENT': 'Event'\n",
    "}\n",
    "\n",
    "exploded_df['Type'] = exploded_df['Type'].map(readable_format).fillna(exploded_df['Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(exploded_df, \"entities_graph_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumenwood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
