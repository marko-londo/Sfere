{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def open_book(filename):\n",
    "    with open(\"../../Resources/Cleaned/\"+filename+\".txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def add_book_to_df(book, book_title):\n",
    "    # Split the book text into paragraphs\n",
    "    paragraphs = book.split('\\n')\n",
    "    \n",
    "    # Clean each paragraph by removing extra whitespace and trimming\n",
    "    paragraphs = [re.sub(r'\\s+', ' ', para.strip()) for para in paragraphs if para.strip()]\n",
    "\n",
    "    # Create a DataFrame with two columns: book title and the paragraph text\n",
    "    df = pd.DataFrame({'Title': [book_title] * len(paragraphs), 'Text': paragraphs})\n",
    "    return df\n",
    "\n",
    "def correct_entity_type(entity_text, correction_dict):\n",
    "    # Normalize the entity text (lowercase, remove extra spaces, handle special chars)\n",
    "    entity_text_normalized = re.sub(r'\\s+', ' ', entity_text).lower().strip(\" '\\\"\")\n",
    "\n",
    "    for category, names in correction_dict.items():\n",
    "        # Normalize and prepare the names in the dictionary\n",
    "        normalized_names = [re.sub(r'\\s+', ' ', name).lower().strip(\" '\\\"\") for name in names]\n",
    "        \n",
    "        if entity_text_normalized in normalized_names:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "def find_entities_in_paragraph(paragraph, entities):\n",
    "    entities_in_paragraph = set()\n",
    "    for ent_text, ent_type in entities:\n",
    "        if ent_text in paragraph:\n",
    "            entities_in_paragraph.add((ent_text, ent_type))\n",
    "    return list(entities_in_paragraph)\n",
    "\n",
    "def dialogue_to_df(text):\n",
    "    pattern = r'\"([^\"]*)\"'\n",
    "    dialogues = re.findall(pattern, text)\n",
    "    df_dialogues = pd.DataFrame(dialogues, columns=['Dialogue'])\n",
    "    return df_dialogues\n",
    "\n",
    "def key_phrase_extractor(text, n=1):\n",
    "    additional_stopwords = {'said', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\"}\n",
    "    custom_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "    # Tokenize the text into words, remove punctuation with regex\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', text))\n",
    "\n",
    "    # Remove stop words and convert to lowercase\n",
    "    words_without_stopwords = [word.lower() for word in words if word.lower() not in custom_stopwords]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = ngrams(words_without_stopwords, n)\n",
    "    n_grams = [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "    # Count the frequency of each n-gram\n",
    "    frequency = Counter(n_grams)\n",
    "\n",
    "    # Get the top N key phrases\n",
    "    N = 100\n",
    "    key_phrases = frequency.most_common(N)\n",
    "\n",
    "    # Create a DataFrame from the top key phrases\n",
    "    df = pd.DataFrame(key_phrases, columns=['phrase', 'count'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_character(entity):\n",
    "    character_types = {'PERSON'}\n",
    "    return entity[1] in character_types\n",
    "\n",
    "def is_location(entity):\n",
    "    location_types = {'LOC'}\n",
    "    return entity[1] in location_types\n",
    "\n",
    "\n",
    "def df_to_csv(df, filename):\n",
    "    df.to_csv(\"../../Resources/Cleaned/\"+filename+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = open_book(\"cosmos_cleaned\")\n",
    "book2 = open_book(\"into_thin_air_cleaned\")\n",
    "book3 = open_book(\"tom_sawyer_cleaned\")\n",
    "book4 = open_book(\"1984_cleaned\")\n",
    "book5 = open_book(\"killing_machine_cleaned\")\n",
    "book6 = open_book(\"androids_cleaned\")\n",
    "book7 = open_book(\"stardust_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_para = add_book_to_df(book1, \"Cosmos\")\n",
    "into_thin_air_para = add_book_to_df(book2, \"Into Thin Air\")\n",
    "tom_sawyer_para = add_book_to_df(book3, \"Tom Sawyer\")\n",
    "para_1984 = add_book_to_df(book4, \"1984\")\n",
    "killing_machine_para = add_book_to_df(book5, \"Killing Machine\")\n",
    "android_para = add_book_to_df(book6, \"Android\")\n",
    "stardust_para = add_book_to_df(book7, \"Stardust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killing_machine_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tom_sawyer_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(cosmos_para, \"cosmos_paragraphs\")\n",
    "df_to_csv(into_thin_air_para, \"into_thin_air_paragraphs\")\n",
    "df_to_csv(tom_sawyer_para, \"tom_sawyer_paragraphs\")\n",
    "df_to_csv(para_1984, \"1984_paragraphs\")\n",
    "df_to_csv(killing_machine_para, \"killing_machine_paragraphs\")\n",
    "df_to_csv(android_para, \"android_paragraphs\")\n",
    "df_to_csv(stardust_para, \"stardust_paragraphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumenwood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
