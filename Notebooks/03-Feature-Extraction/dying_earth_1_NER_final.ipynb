{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_book(filename):\n",
    "    with open(\"../../Resources/Cleaned/\" + filename + \".txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def add_book_to_df(book, book_title):\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    paragraphs = [re.sub(r'\\s+', ' ', para.strip()) for para in paragraphs if para.strip()]\n",
    "    return pd.DataFrame({'Title': [book_title] * len(paragraphs), 'Text': paragraphs})\n",
    "\n",
    "def correct_entity_type(entity_text, entity_type, correction_dict):\n",
    "    entity_text_normalized = re.sub(r'\\s+', ' ', entity_text).lower().strip(\" '\\\"\")\n",
    "    for category, names in correction_dict.items():\n",
    "        normalized_names = [re.sub(r'\\s+', ' ', name).lower().strip(\" '\\\"\") for name in names]\n",
    "        if entity_text_normalized in normalized_names:\n",
    "            return category\n",
    "    return entity_type\n",
    "\n",
    "def is_unwanted_entity(entity_text, entity_type):\n",
    "    return (entity_text, entity_type) in unwanted_entities or entity_type in unwanted_types\n",
    "\n",
    "# Read and Process Book\n",
    "text = open_book(\"de\")\n",
    "df = add_book_to_df(text, \"The Dying Earth\")\n",
    "\n",
    "# NLP Processing\n",
    "def extract_entities(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        corrected_type = correct_entity_type(ent.text, ent.label_, correction_dict)\n",
    "        if not is_unwanted_entity(ent.text, corrected_type):\n",
    "            entities.add((ent.text, corrected_type))\n",
    "    return entities\n",
    "\n",
    "df['Entities'] = df['Text'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def open_book(filename):\n",
    "    with open(\"../../Resources/Cleaned/\"+filename+\".txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def add_book_to_df(book, book_title):\n",
    "    # Split the book text into paragraphs\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    \n",
    "    # Clean each paragraph by removing extra whitespace and trimming\n",
    "    paragraphs = [re.sub(r'\\s+', ' ', para.strip()) for para in paragraphs if para.strip()]\n",
    "\n",
    "    # Create a DataFrame with two columns: book title and the paragraph text\n",
    "    df = pd.DataFrame({'Title': [book_title] * len(paragraphs), 'Text': paragraphs})\n",
    "    return df\n",
    "\n",
    "def correct_entity_type(entity_text, correction_dict):\n",
    "    # Normalize the entity text (lowercase, remove extra spaces, handle special chars)\n",
    "    entity_text_normalized = re.sub(r'\\s+', ' ', entity_text).lower().strip(\" '\\\"\")\n",
    "\n",
    "    for category, names in correction_dict.items():\n",
    "        # Normalize and prepare the names in the dictionary\n",
    "        normalized_names = [re.sub(r'\\s+', ' ', name).lower().strip(\" '\\\"\") for name in names]\n",
    "        \n",
    "        if entity_text_normalized in normalized_names:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "def find_entities_in_paragraph(paragraph, entities):\n",
    "    entities_in_paragraph = set()\n",
    "    for ent_text, ent_type in entities:\n",
    "        if ent_text in paragraph:\n",
    "            entities_in_paragraph.add((ent_text, ent_type))\n",
    "    return list(entities_in_paragraph)\n",
    "\n",
    "def dialogue_to_df(text):\n",
    "    pattern = r'\"([^\"]*)\"'\n",
    "    dialogues = re.findall(pattern, text)\n",
    "    df_dialogues = pd.DataFrame(dialogues, columns=['Dialogue'])\n",
    "    return df_dialogues\n",
    "\n",
    "def key_phrase_extractor(text, n=1):\n",
    "    additional_stopwords = {'said', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\"}\n",
    "    custom_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "    # Tokenize the text into words, remove punctuation with regex\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', text))\n",
    "\n",
    "    # Remove stop words and convert to lowercase\n",
    "    words_without_stopwords = [word.lower() for word in words if word.lower() not in custom_stopwords]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = ngrams(words_without_stopwords, n)\n",
    "    n_grams = [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "    # Count the frequency of each n-gram\n",
    "    frequency = Counter(n_grams)\n",
    "\n",
    "    # Get the top N key phrases\n",
    "    N = 100\n",
    "    key_phrases = frequency.most_common(N)\n",
    "\n",
    "    # Create a DataFrame from the top key phrases\n",
    "    df = pd.DataFrame(key_phrases, columns=['phrase', 'count'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_character(entity):\n",
    "    character_types = {'PERSON'}\n",
    "    return entity[1] in character_types\n",
    "\n",
    "def is_location(entity):\n",
    "    location_types = {'LOC'}\n",
    "    return entity[1] in location_types\n",
    "\n",
    "\n",
    "def df_to_csv(df, filename):\n",
    "    df.to_csv(\"../../Resources/Cleaned/\"+filename+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open_book(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_book_to_df(text, \"The Dying Earth\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Open the book and convert it into a DataFrame\n",
    "text = open_book(\"de\")  # Replace with actual file path if different\n",
    "df = add_book_to_df(text, \"The Dying Earth\")\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace and strip leading/trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Define additional stopwords\n",
    "    additional_stopwords = {\"said\"}\n",
    "\n",
    "    # Update NLP stop words with additional stopwords\n",
    "    for word in additional_stopwords:\n",
    "        nlp.Defaults.stop_words.add(word)\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Filter out tokens that are stop words or punctuation, and lemmatize\n",
    "    filtered_tokens = [token.lemma_ for token in doc if token.text.lower() not in nlp.Defaults.stop_words and not token.is_punct]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Correcting Entity Types Function\n",
    "def correct_entity_type(entity_text, entity_type, correction_dict):\n",
    "    entity_text_normalized = re.sub(r'\\s+', ' ', entity_text).lower().strip(\" '\\\"\")\n",
    "    for category, names in correction_dict.items():\n",
    "        normalized_names = [re.sub(r'\\s+', ' ', name).lower().strip(\" '\\\"\") for name in names]\n",
    "        if entity_text_normalized in normalized_names:\n",
    "            return category\n",
    "    return entity_type\n",
    "\n",
    "# Renaming Entities Function\n",
    "def rename_entity(entity_text, entity_type):\n",
    "    return rename_dict.get((entity_text, entity_type), (entity_text, entity_type))\n",
    "\n",
    "# Filtering Unwanted Entities Function\n",
    "def is_unwanted_entity(entity_text, entity_type):\n",
    "    return (entity_text, entity_type) in unwanted_entities or entity_type in unwanted_types\n",
    "\n",
    "# Process the text with Spacy NLP\n",
    "text = clean_text(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Entities and Apply Corrections\n",
    "final_entities = set()\n",
    "for ent in doc.ents:\n",
    "    corrected_type = correct_entity_type(ent.text, ent.label_, correction_dict)\n",
    "    renamed_text, renamed_type = rename_entity(ent.text, corrected_type)\n",
    "    if not is_unwanted_entity(renamed_text, renamed_type):\n",
    "        final_entities.add((renamed_text, renamed_type))\n",
    "\n",
    "# Function to Extract Matching Entities in Paragraphs\n",
    "def extract_matching_entities(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    paragraph_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        corrected_type = correct_entity_type(ent.text, ent.label_, correction_dict)\n",
    "        renamed_text, renamed_type = rename_entity(ent.text, corrected_type)\n",
    "        if not is_unwanted_entity(renamed_text, renamed_type):\n",
    "            paragraph_entities.add((renamed_text, renamed_type))\n",
    "    return paragraph_entities.intersection(final_entities)\n",
    "\n",
    "# Apply the Function to Each Paragraph\n",
    "df['Entities'] = df['Text'].apply(extract_matching_entities)\n",
    "\n",
    "# Now, df contains your text with the processed entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = pd.DataFrame(final_entities, columns=[\"Entity_Name\", \"Entity_Type\"])\n",
    "ent_df['Entity_Name'] = ent_df['Entity_Name'].str.title()\n",
    "\n",
    "# Remove duplicates\n",
    "de_ent_df = ent_df.drop_duplicates().reset_index(drop=True)\n",
    "de_ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entities(entities_set):\n",
    "    return {(entity, type) for entity, type in entities_set if not (entity == \"Earth\" and type == \"LOC\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# Assuming de_df is your DataFrame and already loaded with the 'Entities' column filled as per your function\n",
    "\n",
    "# Define entity types and their corresponding colors\n",
    "entity_types = ['PERSON', 'LOC', 'ARTIFACT_OBJECT', 'FAC', 'NORP', 'SPELL', 'CREATURE', 'EVENT']\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange', 'purple']\n",
    "color_map = dict(zip(entity_types, colors))\n",
    "\n",
    "# Create a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their type\n",
    "for _, row in df.iterrows():\n",
    "    filtered_entities = filter_entities(row['Entities'])\n",
    "    for entity, entity_type in filtered_entities:\n",
    "        G.add_node(entity, type=entity_type)\n",
    "\n",
    "# Add edges (for simplicity, connecting all entities within the same text, except (\"Earth\", \"LOC\"))\n",
    "for _, row in df.iterrows():\n",
    "    filtered_entities = filter_entities(row['Entities'])\n",
    "    entities = [entity for entity, _ in filtered_entities]\n",
    "    for source, target in combinations(entities, 2):\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "# Position the nodes using a layout to bring outliers closer\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "# Prepare plotly graph\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_color = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_color.append(color_map.get(G.nodes[node]['type'], 'grey'))\n",
    "    node_text.append(node)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=node_color,\n",
    "        line_width=2))\n",
    "\n",
    "# Create layout for the graph\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='<br>Network graph of entities in \"The Dying Earth\"',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
    "fig.update_layout(\n",
    "    width=1000,  # Set the width of the plot\n",
    "    height=1000)\n",
    "\n",
    "# Code to display the graph\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cathedral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
