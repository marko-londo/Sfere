{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def pandas_df(csv):\n",
    "    df = pd.read_csv(r\"../../Resources/Cleaned/\"+csv+\".csv\")\n",
    "    return df\n",
    "\n",
    "def test(data, label):\n",
    "\n",
    "    # Preprocess the text\n",
    "    new_paragraphs = data[\"Text\"].apply(remove_stop_words).values\n",
    "\n",
    "\n",
    "    # Tokenize and pad sequences\n",
    "    new_sequences = tokenizer.texts_to_sequences(new_paragraphs)\n",
    "    new_data = pad_sequences(new_sequences, maxlen=maxlen)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(new_data)\n",
    "    predicted_classes = [1 if prob > 0.5 else 0 for prob in predictions.ravel()]\n",
    "\n",
    "    true_labels = [label] * len(predicted_classes)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(true_labels, predicted_classes, target_names=['Class 0', 'Class 1'])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df = pandas_df(\"dying_earth_corpus\")\n",
    "not_de_df = pandas_df(\"not_dying_earth_corpus\")\n",
    "km_df = pandas_df(\"killing_machine_paragraphs\")\n",
    "android_df = pandas_df(\"android_paragraphs\")\n",
    "stardust_df = pandas_df(\"stardust_paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_df[\"Is_Dying_Earth\"] = 0\n",
    "android_df[\"Is_Dying_Earth\"] = 0\n",
    "stardust_df[\"Is_Dying_Earth\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.concat([de_df, not_de_df], axis=0, ignore_index=True)\n",
    "corpus = corpus.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length = corpus['Text'].apply(len).mean()\n",
    "average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stop word removal to each text in the corpus\n",
    "paragraphs = corpus[\"Text\"].apply(remove_stop_words).values\n",
    "\n",
    "# Tokenization\n",
    "max_words = 25000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "tokenizer.fit_on_texts(tqdm(paragraphs, desc=\"Tokenizing\"))\n",
    "sequences = tokenizer.texts_to_sequences(tqdm(paragraphs, desc=\"Converting to Sequences\"))\n",
    "\n",
    "# Padding sequences\n",
    "maxlen = 438\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Labels\n",
    "labels = corpus[\"Is_Dying_Earth\"].values\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred.ravel()]\n",
    "\n",
    "# Generating the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(android_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(stardust_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(km_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('is_dying_earth_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pred_model = load_model(r\"C:\\Users\\londo\\01\\001\\Repos\\Sfere\\Models\\Keras\\is_dying_earth_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cathedral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
