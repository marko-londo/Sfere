{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23198,"status":"ok","timestamp":1699944331014,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"f1SRI9WydqSZ","outputId":"53708027-2311-4a9d-a4fa-7177f688c4e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Requirement already satisfied: accelerate in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (0.24.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (2.1.0)\n","Requirement already satisfied: huggingface-hub in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: filelock in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n","Requirement already satisfied: requests in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages)\n","WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dontb\\anaconda3\\envs\\cathedral\\lib\\site-packages)\n"]}],"source":["!pip install transformers -U\n","!pip install accelerate -U\n","!pip install torch==2.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21186,"status":"ok","timestamp":1699944352193,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"Rl2TF7ITd4mX","outputId":"7459f043-bc46-49db-f35c-316bd804f251"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10092,"status":"ok","timestamp":1699944362279,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"p3fanlI2eUA7"},"outputs":[],"source":["import pandas as pd\n","import random\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7474,"status":"ok","timestamp":1699944369736,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"L9v9k5LdfZKT"},"outputs":[],"source":["mod_path = \"/content/drive/MyDrive/my_model_folder/generator_model\"\n","tok_path = \"/content/drive/MyDrive/my_model_folder/gen_tokenizer\"\n","\n","model = GPT2LMHeadModel.from_pretrained(mod_path)\n","tokenizer = GPT2Tokenizer.from_pretrained(tok_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":717,"status":"ok","timestamp":1699944370438,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"ZTRGMtMPef9Z"},"outputs":[],"source":["# Load original dataset\n","dataset_path = \"/content/drive/MyDrive/Colab Notebooks/04-Machine-Learning/dying_earth_corpus.txt\"\n","with open(dataset_path, 'r', encoding='utf-8') as file:\n","    original_lines = file.readlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699944370439,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"O6Sm3nuTes0d","outputId":"d039d543-88d1-41ba-cae4-1324803e9a28"},"outputs":[{"name":"stdout","output_type":"stream","text":["TURJAN SAT in his workroom, legs sprawled out from the stool, back against and elbows on the bench. Across the room was a cage; into this Turjan gazed with rueful vexation. The creature in the cage returned the scrutiny with emotions beyond conjecture.<|endoftext|>\n","\n"]}],"source":["print(original_lines[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699944370439,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"wVJE2TSBe3iF"},"outputs":[],"source":["# Data collator used for dynamic padding and combining batch data\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=False\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1699944370810,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"HUYvs5v-gmt0","outputId":"ff992c82-75e4-4a8b-e4b0-5d430ff3b3cd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]}],"source":["train_dataset = TextDataset(\n","    tokenizer=tokenizer, file_path=dataset_path, block_size=400\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699944370810,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"llWnonfcf32j"},"outputs":[],"source":["# Assuming train_dataset is your training dataset instance\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=1,  # or whatever batch size you are using\n","    shuffle=False,  # This ensures data is shuffled each epoch\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2371651,"status":"ok","timestamp":1699946742458,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"QmoJpce01fl8","outputId":"7e26c7af-4203-4782-dc30-79a292a2ea60"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='20440' max='20440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [20440/20440 19:43, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>1.652200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.642400</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.640700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.649800</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>1.600900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.572300</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>1.585100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.608200</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>1.540600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.563500</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>1.526700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.559300</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>1.500500</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.508100</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>1.512000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.506300</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>1.465200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.483900</td>\n","    </tr>\n","    <tr>\n","      <td>4750</td>\n","      <td>1.480100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.465800</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>1.463500</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.451300</td>\n","    </tr>\n","    <tr>\n","      <td>5750</td>\n","      <td>1.436500</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.439000</td>\n","    </tr>\n","    <tr>\n","      <td>6250</td>\n","      <td>1.444300</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.431500</td>\n","    </tr>\n","    <tr>\n","      <td>6750</td>\n","      <td>1.396300</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.423600</td>\n","    </tr>\n","    <tr>\n","      <td>7250</td>\n","      <td>1.403100</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.364700</td>\n","    </tr>\n","    <tr>\n","      <td>7750</td>\n","      <td>1.374200</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.402000</td>\n","    </tr>\n","    <tr>\n","      <td>8250</td>\n","      <td>1.414000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.343200</td>\n","    </tr>\n","    <tr>\n","      <td>8750</td>\n","      <td>1.384900</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.360600</td>\n","    </tr>\n","    <tr>\n","      <td>9250</td>\n","      <td>1.370600</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.351500</td>\n","    </tr>\n","    <tr>\n","      <td>9750</td>\n","      <td>1.329000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.362500</td>\n","    </tr>\n","    <tr>\n","      <td>10250</td>\n","      <td>1.348300</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.288800</td>\n","    </tr>\n","    <tr>\n","      <td>10750</td>\n","      <td>1.329100</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.355400</td>\n","    </tr>\n","    <tr>\n","      <td>11250</td>\n","      <td>1.344800</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.319400</td>\n","    </tr>\n","    <tr>\n","      <td>11750</td>\n","      <td>1.320400</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.303600</td>\n","    </tr>\n","    <tr>\n","      <td>12250</td>\n","      <td>1.298000</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.293000</td>\n","    </tr>\n","    <tr>\n","      <td>12750</td>\n","      <td>1.279500</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>1.295300</td>\n","    </tr>\n","    <tr>\n","      <td>13250</td>\n","      <td>1.295700</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>1.284000</td>\n","    </tr>\n","    <tr>\n","      <td>13750</td>\n","      <td>1.301400</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>1.274900</td>\n","    </tr>\n","    <tr>\n","      <td>14250</td>\n","      <td>1.278000</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>1.258500</td>\n","    </tr>\n","    <tr>\n","      <td>14750</td>\n","      <td>1.276100</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>1.274000</td>\n","    </tr>\n","    <tr>\n","      <td>15250</td>\n","      <td>1.270500</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>1.270600</td>\n","    </tr>\n","    <tr>\n","      <td>15750</td>\n","      <td>1.262200</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>1.252800</td>\n","    </tr>\n","    <tr>\n","      <td>16250</td>\n","      <td>1.262200</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>1.262200</td>\n","    </tr>\n","    <tr>\n","      <td>16750</td>\n","      <td>1.249300</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>1.251700</td>\n","    </tr>\n","    <tr>\n","      <td>17250</td>\n","      <td>1.243200</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>1.228800</td>\n","    </tr>\n","    <tr>\n","      <td>17750</td>\n","      <td>1.246700</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>1.250200</td>\n","    </tr>\n","    <tr>\n","      <td>18250</td>\n","      <td>1.249200</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>1.250400</td>\n","    </tr>\n","    <tr>\n","      <td>18750</td>\n","      <td>1.236900</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>1.235400</td>\n","    </tr>\n","    <tr>\n","      <td>19250</td>\n","      <td>1.232200</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>1.240600</td>\n","    </tr>\n","    <tr>\n","      <td>19750</td>\n","      <td>1.225400</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>1.235500</td>\n","    </tr>\n","    <tr>\n","      <td>20250</td>\n","      <td>1.249800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='20440' max='20440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [20440/20440 19:37, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>1.232000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.268200</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.270400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.272000</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>1.224000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.228900</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>1.245700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.222400</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>1.203600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.213700</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>1.196400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.213600</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>1.171800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.184800</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>1.181800</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.188300</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>1.163600</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.132500</td>\n","    </tr>\n","    <tr>\n","      <td>4750</td>\n","      <td>1.165800</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.145200</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>1.140800</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.125300</td>\n","    </tr>\n","    <tr>\n","      <td>5750</td>\n","      <td>1.131600</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.144500</td>\n","    </tr>\n","    <tr>\n","      <td>6250</td>\n","      <td>1.116600</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.096300</td>\n","    </tr>\n","    <tr>\n","      <td>6750</td>\n","      <td>1.120500</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.113000</td>\n","    </tr>\n","    <tr>\n","      <td>7250</td>\n","      <td>1.111500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.096400</td>\n","    </tr>\n","    <tr>\n","      <td>7750</td>\n","      <td>1.084300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.089200</td>\n","    </tr>\n","    <tr>\n","      <td>8250</td>\n","      <td>1.095700</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.082800</td>\n","    </tr>\n","    <tr>\n","      <td>8750</td>\n","      <td>1.062100</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.071900</td>\n","    </tr>\n","    <tr>\n","      <td>9250</td>\n","      <td>1.060300</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.052700</td>\n","    </tr>\n","    <tr>\n","      <td>9750</td>\n","      <td>1.056000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.052500</td>\n","    </tr>\n","    <tr>\n","      <td>10250</td>\n","      <td>1.052800</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.031700</td>\n","    </tr>\n","    <tr>\n","      <td>10750</td>\n","      <td>1.029300</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.061000</td>\n","    </tr>\n","    <tr>\n","      <td>11250</td>\n","      <td>1.044900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.008000</td>\n","    </tr>\n","    <tr>\n","      <td>11750</td>\n","      <td>1.031900</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.027400</td>\n","    </tr>\n","    <tr>\n","      <td>12250</td>\n","      <td>1.021200</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.026600</td>\n","    </tr>\n","    <tr>\n","      <td>12750</td>\n","      <td>1.006500</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>1.001800</td>\n","    </tr>\n","    <tr>\n","      <td>13250</td>\n","      <td>1.022300</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>1.021700</td>\n","    </tr>\n","    <tr>\n","      <td>13750</td>\n","      <td>0.985900</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>1.012100</td>\n","    </tr>\n","    <tr>\n","      <td>14250</td>\n","      <td>0.994400</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.992600</td>\n","    </tr>\n","    <tr>\n","      <td>14750</td>\n","      <td>0.980200</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.991200</td>\n","    </tr>\n","    <tr>\n","      <td>15250</td>\n","      <td>1.011700</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.976800</td>\n","    </tr>\n","    <tr>\n","      <td>15750</td>\n","      <td>0.977800</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.975200</td>\n","    </tr>\n","    <tr>\n","      <td>16250</td>\n","      <td>1.017400</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.984500</td>\n","    </tr>\n","    <tr>\n","      <td>16750</td>\n","      <td>0.958000</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.968600</td>\n","    </tr>\n","    <tr>\n","      <td>17250</td>\n","      <td>0.987600</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>0.969100</td>\n","    </tr>\n","    <tr>\n","      <td>17750</td>\n","      <td>0.978100</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.953500</td>\n","    </tr>\n","    <tr>\n","      <td>18250</td>\n","      <td>0.992600</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>0.980300</td>\n","    </tr>\n","    <tr>\n","      <td>18750</td>\n","      <td>0.955000</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>0.968700</td>\n","    </tr>\n","    <tr>\n","      <td>19250</td>\n","      <td>0.978400</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>0.973600</td>\n","    </tr>\n","    <tr>\n","      <td>19750</td>\n","      <td>0.959400</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>0.962100</td>\n","    </tr>\n","    <tr>\n","      <td>20250</td>\n","      <td>0.964800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=20440, training_loss=1.0714999936098222, metrics={'train_runtime': 1177.1575, 'train_samples_per_second': 17.364, 'train_steps_per_second': 17.364, 'total_flos': 4172507136000000.0, 'train_loss': 1.0714999936098222, 'epoch': 20.0})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-dying-earth\",  # Output directory for model checkpoints\n","    overwrite_output_dir=True,  # Overwrite the content of the output directory\n","    num_train_epochs=20,  # Number of epochs (will be overridden by max_steps)\n","    per_device_train_batch_size=1,  # Batch size per device\n","    save_steps=1_000,  # Saving frequency for model checkpoints\n","    save_total_limit=5,  # Max number of saved checkpoints\n","    logging_dir='./logs',  # Directory for storing logs\n","    logging_steps=250,  # Logging frequency\n","    load_best_model_at_end=False,  # Load the best model at the end of training\n","    metric_for_best_model='loss',  # Metric for evaluating the best model\n","    greater_is_better=False,  # Lower loss indicates a better model\n","    fp16=True,  # Use mixed precision training if GPU supports it\n","    learning_rate=1e-5,  # Learning rate\n","    weight_decay=0.01,  # Weight decay for L2 regularization\n",")\n","\n","# Create a new instance of Trainer or reset the existing one\n","trainer = Trainer(\n","    model=model,  # assuming 'model' is your pre-trained GPT-2 model\n","    args=training_args,  # 'training_args' should be your TrainingArguments instance\n","    data_collator=data_collator,  # 'data_collator' for handling batching\n","    train_dataset=train_dataset,  # your training dataset\n",")\n","\n","# Start training\n","trainer.train()\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699949370384,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"sfnttFfO0q9x"},"outputs":[],"source":["# Creating a list of 100 prompt starters following the specified parameters\n","\n","prompt_starters = []\n","\n","# Adding prompts focused on nature themes\n","nature_themed_prompts = [\n","    \"Trees whispered in\",\n","    \"Sunlight bathed the\",\n","    \"Moonlight shone on\",\n","    \"Stars twinkled above\",\n","    \"Winds howled through\",\n","    \"Rivers murmured in\",\n","    \"Leaves rustled under\",\n","    \"Mountains loomed over\",\n","    \"Fog enveloped the\",\n","    \"Rain pattered on\",\n","    \"Snow blanketed the\",\n","    \"Clouds drifted over\",\n","    \"Dew glistened on\",\n","    \"Thunder echoed in\",\n","    \"Birds sang in\",\n","    \"Flowers bloomed in\",\n","    \"Vines climbed the\",\n","    \"Mist shrouded the\",\n","    \"Fireflies glowed in\",\n","    \"Crickets chirped in\",\n","    \"Frost covered the\",\n","    \"Breezes caressed the\",\n","    \"Waves crashed against\",\n","    \"Seas roared around\"\n","]\n","\n","# Adding mystical prompts\n","mystical_prompts = [\n","    \"Magic swirled around\",\n","    \"Spirits danced in\",\n","    \"Auras glimmered in\",\n","    \"Portals opened in\",\n","    \"Visions appeared in\",\n","    \"Ghosts roamed the\",\n","    \"Witches chanted in\",\n","    \"Dragons soared over\",\n","    \"Enchantments wove through\",\n","    \"Sorcerers summoned in\",\n","    \"Runes glowed on\",\n","    \"Curses lingered in\",\n","    \"Golems trudged through\",\n","    \"Phantoms floated over\",\n","    \"Alchemy transformed the\",\n","    \"Crystals hummed in\",\n","    \"Fairies flitted around\",\n","    \"Griffins perched on\",\n","    \"Elixirs bubbled in\",\n","    \"Charms activated in\"\n","]\n","\n","# Adding prompts that are the beginning of a scenic descriptive passage\n","scenic_prompts = [\n","    \"Valleys stretched below\",\n","    \"Sunsets painted the\",\n","    \"Dawns broke over\",\n","    \"Horizons merged at\",\n","    \"Skylines dominated the\",\n","    \"Landscapes changed beyond\",\n","    \"Sceneries shifted near\",\n","    \"Views expanded towards\",\n","    \"Panoramas unfolded across\",\n","    \"Tableaus formed in\"\n","]\n","\n","# Adding prompts containing references to the Dying Earth series\n","dying_earth_prompts = [\n","    \"Cugel navigated through\",\n","    \"Rhialto pondered in\",\n","    \"Iucounu smirked at\",\n","    \"T'sais wandered in\",\n","    \"Turjan studied in\",\n","    \"Liane gazed upon\",\n","    \"Guyal sought answers in\",\n","    \"Firx scampered across\",\n","    \"Deodarr roamed the\",\n","    \"Ildefonse plotted in\",\n","    \"Zaraide meditated in\",\n","    \"Faide pursued through\",\n","    \"Pharesm guarded the\",\n","    \"Pelgrane flew over\",\n","    \"Baleful stars glinted above\",\n","    \"Scaum river flowed by\",\n","    \"Vermoulian schemed in\",\n","    \"Derwe Coreme admired\",\n","    \"Grue lurked in\",\n","    \"Sandestins worked in\",\n","    \"Spell-Wrights chanted in\",\n","    \"Mazirian's magic wove around\",\n","    \"Dolm's curse lingered in\",\n","    \"Dying Earth's sun set on\",\n","    \"Vancian mysteries unfolded in\"\n","]\n","\n","# Combining all the prompts\n","prompt_starters.extend(nature_themed_prompts)\n","prompt_starters.extend(mystical_prompts)\n","prompt_starters.extend(scenic_prompts)\n","prompt_starters.extend(dying_earth_prompts)\n","\n","# Ensuring we have 100 unique prompts\n","unique_prompt_starters = list(set(prompt_starters))\n","\n","# Expanding the existing lists and adding new prompts\n","expanded_nature_themed_prompts = [\n","    \"Trees whispered in\", \"Sunlight bathed the\", \"Moonlight shone on\",\n","    \"Stars twinkled above\", \"Winds howled through\", \"Rivers murmured in\",\n","    \"Leaves rustled under\", \"Mountains loomed over\", \"Fog enveloped the\",\n","    \"Rain pattered on\", \"Snow blanketed the\", \"Clouds drifted over\",\n","    \"Dew glistened on\", \"Thunder echoed in\", \"Birds sang in\", \"Flowers bloomed in\",\n","    \"Vines climbed the\", \"Mist shrouded the\", \"Fireflies glowed in\",\n","    \"Crickets chirped in\", \"Frost covered the\", \"Breezes caressed the\",\n","    \"Waves crashed against\", \"Seas roared around\", \"Brooks babbled through\",\n","    \"Hills rolled under\", \"Valleys yawned below\", \"Glaciers crept over\",\n","    \"Oceans stretched beyond\", \"Storms raged over\", \"Forests stood around\",\n","    \"Meadows spread beneath\", \"Lakes shimmered under\", \"Streams trickled through\",\n","    \"Deserts baked under\", \"Canyons echoed with\", \"Prairies swayed in\",\n","    \"Islands emerged from\", \"Volcanoes erupted over\", \"Marshes squelched under\"\n","]\n","\n","expanded_mystical_prompts = [\n","    \"Magic swirled around\", \"Spirits danced in\", \"Auras glimmered in\",\n","    \"Portals opened in\", \"Visions appeared in\", \"Ghosts roamed the\",\n","    \"Witches chanted in\", \"Dragons soared over\", \"Enchantments wove through\",\n","    \"Sorcerers summoned in\", \"Runes glowed on\", \"Curses lingered in\",\n","    \"Golems trudged through\", \"Phantoms floated over\", \"Alchemy transformed the\",\n","    \"Crystals hummed in\", \"Fairies flitted around\", \"Griffins perched on\",\n","    \"Elixirs bubbled in\", \"Charms activated in\", \"Oracles foretold in\",\n","    \"Necromancers conjured in\", \"Demons lurked in\", \"Angels descended on\",\n","    \"Totems stood in\", \"Amulets shone under\", \"Wizards pondered in\",\n","    \"Spells echoed through\", \"Shamans communicated with\", \"Chimeras roamed the\"\n","]\n","\n","expanded_scenic_prompts = [\n","    \"Valleys stretched below\", \"Sunsets painted the\", \"Dawns broke over\",\n","    \"Horizons merged at\", \"Skylines dominated the\", \"Landscapes changed beyond\",\n","    \"Sceneries shifted near\", \"Views expanded towards\", \"Panoramas unfolded across\",\n","    \"Tableaus formed in\", \"Vistas revealed the\", \"Sunrises lit up\",\n","    \"Twilights settled over\", \"Tundras lay under\", \"Savannas spread across\",\n","    \"Oases appeared in\", \"Fjords cut through\", \"Plateaus rose above\",\n","    \"Badlands sprawled under\", \"Steppes swept across\"\n","]\n","\n","expanded_dying_earth_prompts = [\n","    \"Cugel navigated through\", \"Rhialto pondered in\", \"Iucounu smirked at\",\n","    \"T'sais wandered in\", \"Turjan studied in\", \"Liane gazed upon\",\n","    \"Guyal sought answers in\", \"Firx scampered across\", \"Deodarr roamed the\",\n","    \"Ildefonse plotted in\", \"Zaraide meditated in\", \"Faide pursued through\",\n","    \"Pharesm guarded the\", \"Pelgrane flew over\", \"Baleful stars glinted above\",\n","    \"Scaum river flowed by\", \"Vermoulian schemed in\", \"Derwe Coreme admired\",\n","    \"Grue lurked in\", \"Sandestins worked in\", \"Spell-Wrights chanted in\",\n","    \"Mazirian's magic wove around\", \"Dolm's curse lingered in\", \"Dying Earth's sun set on\",\n","    \"Vancian mysteries unfolded in\", \"Zamilon plotted beneath\", \"Klarkash-Ton brooded over\",\n","    \"Blikdak delved into\", \"Pilgrims traveled to\", \"Mummers acted in\"\n","]\n","\n","# Combining all the prompts into a single list\n","all_prompts = (expanded_nature_themed_prompts + expanded_mystical_prompts +\n","               expanded_scenic_prompts + expanded_dying_earth_prompts)\n","\n","# Ensuring uniqueness and sufficient number of prompts\n","unique_prompt_starters = list(set(all_prompts))\n","while len(unique_prompt_starters) < 100:\n","    unique_prompt_starters.append(f\"New unique prompt {len(unique_prompt_starters)+1}\")\n","\n","# Checking the total number of prompts and displaying a few examples\n","len(unique_prompt_starters), unique_prompt_starters[:5]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","with open('prompt_starters_list.pickle', 'wb') as file:\n","    pickle.dump(prompt_starters, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3348,"status":"ok","timestamp":1699950598952,"user":{"displayName":"Marko Varela","userId":"10078092135085465155"},"user_tz":300},"id":"HEWUrocLq2QC","outputId":"a370054e-b72f-4c00-b564-b5dad9c18e7c"},"outputs":[{"ename":"NameError","evalue":"name 'random' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\dontb\\01\\001\\Repos\\Dying-Earth\\Notebooks\\04-Machine-Learning\\de_generator_fine-tuning.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dontb/01/001/Repos/Dying-Earth/Notebooks/04-Machine-Learning/de_generator_fine-tuning.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(prompt_starters)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dontb/01/001/Repos/Dying-Earth/Notebooks/04-Machine-Learning/de_generator_fine-tuning.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dontb/01/001/Repos/Dying-Earth/Notebooks/04-Machine-Learning/de_generator_fine-tuning.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Move input_ids to the same device as the model\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"]}],"source":["prompt = random.choice(prompt_starters)\n","input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","# Move input_ids to the same device as the model\n","input_ids = input_ids.to(model.device)\n","\n","max_length = len(input_ids.tolist()[0]) + 600  # Increase max length for longer outputs\n","\n","# Generate and decode text\n","from transformers import set_seed\n","\n","# Optional: Set a seed for reproducibility\n","\n","# Adjusting generation parameters\n","output = model.generate(\n","    input_ids,\n","    max_length=max_length,  # Updated max length\n","    do_sample=True,        # Enable sampling\n","    temperature=0.5,       # Adjust the temperature if needed\n","    top_k=60,              # Use top-k sampling\n","    top_p=0.75,            # Use top-p (nucleus) sampling\n","    pad_token_id=tokenizer.eos_token_id,\n","    attention_mask=input_ids.new_ones(input_ids.shape)\n",")\n","\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMXzSqhTtDHdhcbgytR8gr6","gpuType":"V100","machine_shape":"hm","provenance":[{"file_id":"1t6jxguE6RSMgQWxRVFlbrHHTLwEFFTcU","timestamp":1699950666008}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
